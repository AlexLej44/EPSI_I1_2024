{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StlA3Uc_3bKu"
      },
      "source": [
        "# Importation des Librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "_1bDxeGF3bKx"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import csv\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t1Y0Hdg3bK0"
      },
      "source": [
        "## 1. Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR8gJjlm3bK2"
      },
      "source": [
        "1.1 Importer les données Train et Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "_uuid": "f66eed17ca77cd4973b8f00c510c5d98e4dc2af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY020U_B3bK3",
        "outputId": "2ba9b430-2ac1-43ff-b9fb-893780875485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset:\n",
            "   PassengerId  Survived  Pclass  Sex  Age  Fare  Embarked  IsAlone  Title\n",
            "0            1         0       3    1    1     0         0        0      1\n",
            "1            2         1       1    0    2     3         1        0      3\n",
            "2            3         1       3    0    1     1         0        1      2\n",
            "3            4         1       1    0    2     3         0        0      3\n",
            "4            5         0       3    1    2     1         0        1      1\n",
            "\n",
            "Test Dataset:\n",
            "   PassengerId  Pclass  Sex  Age  Fare  Embarked  IsAlone  Title\n",
            "0          892       3    1    2     0         2        1      1\n",
            "1          893       3    0    2     0         0        0      3\n",
            "2          894       2    1    3     1         2        1      1\n",
            "3          895       3    1    1     1         0        1      1\n",
            "4          896       3    0    1     1         0        0      3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Définir les chemins des fichiers CSV (à adapter selon l'emplacement des fichiers téléchargés)\n",
        "train_path = '/content/train_clean.csv'\n",
        "test_path = '/content/test_clean.csv'\n",
        "\n",
        "# Vérifier si les fichiers existent\n",
        "if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
        "    print\n",
        "else:\n",
        "    # Charger les fichiers CSV\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_test = pd.read_csv(test_path)\n",
        "\n",
        "    # Afficher les 5 premières lignes pour vérification\n",
        "    print(\"Train Dataset:\")\n",
        "    print(df_train.head())\n",
        "    print(\"\\nTest Dataset:\")\n",
        "    print(df_test.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYk1ky6B3bK3"
      },
      "source": [
        "1.2 Depuis le Dataframe train, charger les features d'apprentissage dans un array numpy X_alltrain, et les labels (données à prévoir) dans un array numpy y_alltrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha-z4ZlX3bK4",
        "outputId": "2ebf3ab1-cf87-46d9-e391-08dbf16f4b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_alltrain shape: (891, 7)\n",
            "y_alltrain shape: (891,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Définir le chemin du fichier\n",
        "train_path = \"/content/train_clean.csv\"\n",
        "\n",
        "# Vérifier si le fichier existe\n",
        "if os.path.exists(train_path):\n",
        "    df_train = pd.read_csv(train_path)\n",
        "\n",
        "    # Extraire les features et labels\n",
        "    X_alltrain = df_train.drop(columns=[\"Survived\", \"PassengerId\"]).values\n",
        "    y_alltrain = df_train[\"Survived\"].values\n",
        "\n",
        "    print(f\"X_alltrain shape: {X_alltrain.shape}\")\n",
        "    print(f\"y_alltrain shape: {y_alltrain.shape}\")\n",
        "\n",
        "else:\n",
        "    print\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2TY-C8z3bK4"
      },
      "source": [
        "1.3 Séparer les features et les labels en deux parties (train et dev), en attribuant 10% des exemples aux données de dev. afficher les nombres de lignes et de colonnes pour les 4 arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "_uuid": "4444e870e198520bbbd400b780cafb88571feb8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vitfi4c3bK5",
        "outputId": "9698e3d5-1384-4e1e-dfb6-29e7834d211e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (801, 7)\n",
            "X_dev shape: (90, 7)\n",
            "y_train shape: (801,)\n",
            "y_dev shape: (90,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Séparation en 90% train / 10% dev\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X_alltrain, y_alltrain, test_size=0.1, random_state=42, stratify=y_alltrain)\n",
        "\n",
        "# Afficher les dimensions des 4 arrays\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_dev shape: {X_dev.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_dev shape: {y_dev.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QPPaqjz3bK5"
      },
      "source": [
        "1.4 Afficher les 10 premières lignes de features et les 10 premiers labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "_uuid": "4444e870e198520bbbd400b780cafb88571feb8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL3tmo_43bK6",
        "outputId": "1b810245-0b89-47ad-ac40-b9d7232f9e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 premières lignes de X_train:\n",
            "[[3 1 0 3 0 0 1]\n",
            " [1 0 0 3 1 0 2]\n",
            " [3 1 1 2 2 1 1]\n",
            " [3 1 1 1 0 1 1]\n",
            " [3 1 1 0 0 1 1]\n",
            " [1 1 2 3 0 0 1]\n",
            " [1 1 3 3 0 1 1]\n",
            " [1 0 3 3 1 0 2]\n",
            " [3 1 1 1 0 1 1]\n",
            " [1 0 1 3 0 0 2]]\n",
            "\n",
            "10 premiers labels de y_train:\n",
            "[0 1 0 0 0 0 0 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "# Afficher les 10 premières lignes des features\n",
        "print(\"10 premières lignes de X_train:\")\n",
        "print(X_train[:10])\n",
        "\n",
        "# Afficher les 10 premiers labels\n",
        "print(\"\\n10 premiers labels de y_train:\")\n",
        "print(y_train[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuSFvE_X3bK6"
      },
      "source": [
        "## 2. Définition du réseau de neurone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weIEbo4w3bK7"
      },
      "source": [
        "2.1 Définir et instancier une classe Titanic Model avec les caractéristiques suivantes :\n",
        "- deux  couches cachées de 50 neurones.\n",
        "- deux classes en sortie : Survivant ou non\n",
        "- Des fonctions d'activation RELU\n",
        "- un dropout paramétrable pour les 2 couches cachées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HELwiP9K3bK8",
        "outputId": "d23d823a-15e8-43b9-9203-d7819791a934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de minibatchs par epoch: 16\n",
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car CrossEntropyLoss inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "batch_size = 50  # Taille du minibatch\n",
        "\n",
        "# Calcul du nombre de boucles par epoch\n",
        "num_batches_per_epoch = len(X_train) // batch_size\n",
        "print(f\"Nombre de minibatchs par epoch: {num_batches_per_epoch}\")\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)\n",
        "\n",
        "# Définition de la fonction de coût (Cross Entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Définition de l'optimiseur Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykhi_uve3bK8"
      },
      "source": [
        "2.2 : Définir des paramètres de nombre d'epochs (50) et de learning_rate (0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "_uuid": "9d5641b90eccab8e9643e288c767984a539d63e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzGKyG7K3bK8",
        "outputId": "64fa8584-a31f-4a70-e7a4-2f5114d79078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones)\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car on utilisera CrossEntropyLoss qui inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1HvNC43bK8"
      },
      "source": [
        "2.3 définir la taille du minibatch à 50. En déduire le nombre de boucle pour chaque epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "_uuid": "9d5641b90eccab8e9643e288c767984a539d63e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT2GAhaS3bK8",
        "outputId": "530a91b2-cf0e-4e18-b949-6791899635fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de minibatchs par epoch: 16\n",
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones)\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car on utilisera CrossEntropyLoss qui inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "batch_size = 50  # Taille du minibatch\n",
        "\n",
        "# Calcul du nombre de boucles par epoch\n",
        "num_batches_per_epoch = len(X_train) // batch_size\n",
        "print(f\"Nombre de minibatchs par epoch: {num_batches_per_epoch}\")\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6_Op4I33bK8"
      },
      "source": [
        "2.4 Définir un fonction de coût de type CrossEntropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "_uuid": "938adf17cd24b2754282ca060cfb64a9df87c102",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LS7LCkt3bK8",
        "outputId": "f4f49146-6d06-48b2-a593-2f963924ae13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de minibatchs par epoch: 16\n",
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones)\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car on utilisera CrossEntropyLoss qui inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "batch_size = 50  # Taille du minibatch\n",
        "\n",
        "# Calcul du nombre de boucles par epoch\n",
        "num_batches_per_epoch = len(X_train) // batch_size\n",
        "print(f\"Nombre de minibatchs par epoch: {num_batches_per_epoch}\")\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)\n",
        "\n",
        "# Définition de la fonction de coût (Cross Entropy)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q00QvJAm3bK9"
      },
      "source": [
        "2.5. Définir un optimizer de type Adam, sans oublier le learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "_uuid": "938adf17cd24b2754282ca060cfb64a9df87c102",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfDi1UpO3bK9",
        "outputId": "cae9211c-2748-4968-ec9c-716791e9a715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de minibatchs par epoch: 16\n",
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones)\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car on utilisera CrossEntropyLoss qui inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "batch_size = 50  # Taille du minibatch\n",
        "\n",
        "# Calcul du nombre de boucles par epoch\n",
        "num_batches_per_epoch = len(X_train) // batch_size\n",
        "print(f\"Nombre de minibatchs par epoch: {num_batches_per_epoch}\")\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)\n",
        "\n",
        "# Définition de la fonction de coût (Cross Entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Définition de l'optimiseur Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkP3NNEI3bK9"
      },
      "source": [
        "## 3. Apprentissage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK9akp7X3bK-"
      },
      "source": [
        "3.1 exécuter l'apprentissage du modèle.\n",
        "- Créer une boucle sur les epochs, qui contient elel-même une boucle sur les minibatchs.\n",
        "- A chaque nouvelle itération sur les epochs, mélanger les données avec la méthode shuffle.\n",
        "- Tous les 5 epochs afficher la valeur de la fonction de cout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "_uuid": "9f2d8485cc5aac6b03a1aca4c163b5c6f5cd4259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHewN_bA3bK-",
        "outputId": "44280ca2-d4c5-4bc1-ef6e-2ab81102411f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de minibatchs par epoch: 16\n",
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n",
            "Epoch [5/50], Loss: 0.0106\n",
            "Epoch [10/50], Loss: 0.2732\n",
            "Epoch [15/50], Loss: 0.8971\n",
            "Epoch [20/50], Loss: 0.3889\n",
            "Epoch [25/50], Loss: 0.6020\n",
            "Epoch [30/50], Loss: 0.1255\n",
            "Epoch [35/50], Loss: 0.2926\n",
            "Epoch [40/50], Loss: 0.1863\n",
            "Epoch [45/50], Loss: 0.1474\n",
            "Epoch [50/50], Loss: 0.7303\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car CrossEntropyLoss inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "batch_size = 50  # Taille du minibatch\n",
        "\n",
        "# Calcul du nombre de boucles par epoch\n",
        "num_batches_per_epoch = len(X_train) // batch_size\n",
        "print(f\"Nombre de minibatchs par epoch: {num_batches_per_epoch}\")\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)\n",
        "\n",
        "# Définition de la fonction de coût (Cross Entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Définition de l'optimiseur Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Conversion des données en tenseurs PyTorch\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "# Création du DataLoader pour gérer les batchs\n",
        "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Entraînement du modèle\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Afficher la perte tous les 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGQkdFzc3bK-"
      },
      "source": [
        "3.2 Calculer la précision de la prévision sur les données dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "_uuid": "b30b4c578b3ddd40ec9f63e19396b045c5ac0dd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdYBNz2W3bK-",
        "outputId": "7a0da94c-1599-469f-9a03-60a0442f7937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de minibatchs par epoch: 16\n",
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n",
            "Epoch [5/50], Loss: 0.9063\n",
            "Epoch [10/50], Loss: 0.0108\n",
            "Epoch [15/50], Loss: 0.8504\n",
            "Epoch [20/50], Loss: 0.0632\n",
            "Epoch [25/50], Loss: 0.1963\n",
            "Epoch [30/50], Loss: 0.0943\n",
            "Epoch [35/50], Loss: 0.9530\n",
            "Epoch [40/50], Loss: 0.9914\n",
            "Epoch [45/50], Loss: 0.8517\n",
            "Epoch [50/50], Loss: 0.3507\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car CrossEntropyLoss inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "batch_size = 50  # Taille du minibatch\n",
        "\n",
        "# Calcul du nombre de boucles par epoch\n",
        "num_batches_per_epoch = len(X_train) // batch_size\n",
        "print(f\"Nombre de minibatchs par epoch: {num_batches_per_epoch}\")\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)\n",
        "\n",
        "# Définition de la fonction de coût (Cross Entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Définition de l'optimiseur Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Conversion des données en tenseurs PyTorch\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "# Création du DataLoader pour gérer les batchs\n",
        "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Entraînement du modèle\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Afficher la perte tous les 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbjcUTKg3bK-"
      },
      "source": [
        "3.3 Calculer prévisions sur les données de tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "_uuid": "10054399a4160803528137e2ebf60b73394b6007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0yzZksw3bK_",
        "outputId": "618b5669-9040-4c60-a921-0104c6ea565d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n",
            "Epoch [5/50], Loss: 0.9951\n",
            "Epoch [10/50], Loss: 0.0054\n",
            "Epoch [15/50], Loss: 0.0685\n",
            "Epoch [20/50], Loss: 0.2030\n",
            "Epoch [25/50], Loss: 0.8004\n",
            "Epoch [30/50], Loss: 0.5760\n",
            "Epoch [35/50], Loss: 0.2443\n",
            "Epoch [40/50], Loss: 0.1719\n",
            "Epoch [45/50], Loss: 1.7841\n",
            "Epoch [50/50], Loss: 0.2171\n",
            "Précision sur les données de validation: 76.67%\n",
            "Prédictions sur les données de test générées.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger les données\n",
        "train_path = '/content/train_clean.csv'\n",
        "test_path = '/content/test_clean.csv'\n",
        "\n",
        "if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_test = pd.read_csv(test_path)\n",
        "\n",
        "    # Séparation des features et labels\n",
        "    X_alltrain = df_train.drop(columns=[\"Survived\", \"PassengerId\"]).values\n",
        "    y_alltrain = df_train[\"Survived\"].values\n",
        "\n",
        "    # Séparation en train/dev\n",
        "    X_train, X_dev, y_train, y_dev = train_test_split(X_alltrain, y_alltrain, test_size=0.1, random_state=42, stratify=y_alltrain)\n",
        "\n",
        "    # Extraction des features pour le test\n",
        "    X_test = df_test.drop(columns=[\"PassengerId\"]).values\n",
        "else:\n",
        "    raise FileNotFoundError(\"Les fichiers de données ne sont pas trouvés. Assurez-vous de les télécharger dans Colab.\")\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car CrossEntropyLoss inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "batch_size = 50  # Taille du minibatch\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)\n",
        "\n",
        "# Définition de la fonction de coût (Cross Entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Définition de l'optimiseur Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Conversion des données en tenseurs PyTorch\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_dev_tensor = torch.tensor(X_dev, dtype=torch.float32)\n",
        "y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Création du DataLoader pour gérer les batchs\n",
        "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Entraînement du modèle\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Afficher la perte tous les 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Évaluation du modèle sur les données de validation\n",
        "def evaluate_model(model, X_dev, y_dev):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_dev)\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        accuracy = (predictions == y_dev).float().mean().item()\n",
        "    return accuracy\n",
        "\n",
        "dev_accuracy = evaluate_model(model, X_dev_tensor, y_dev_tensor)\n",
        "print(f\"Précision sur les données de validation: {dev_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Prédictions sur les données de test\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "\n",
        "print(\"Prédictions sur les données de test générées.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsDKhB_U3bK_"
      },
      "source": [
        "3.4 Générer le fichier résultat et l'envoyer sur kaggle\n",
        "Quel est votre score et votre classement ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "_uuid": "901d7d529bfe936d8c5091e2a73ffc9d1c5da8ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbgUKreZ3bK_",
        "outputId": "c0bc848f-24ce-42bb-8cd6-1e7a6f739ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TitanicModel(\n",
            "  (fc1): Linear(in_features=7, out_features=50, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n",
            "Epoch [5/50], Loss: 1.0703\n",
            "Epoch [10/50], Loss: 0.3937\n",
            "Epoch [15/50], Loss: 0.5924\n",
            "Epoch [20/50], Loss: 0.0421\n",
            "Epoch [25/50], Loss: 0.6659\n",
            "Epoch [30/50], Loss: 0.1701\n",
            "Epoch [35/50], Loss: 0.3454\n",
            "Epoch [40/50], Loss: 1.1207\n",
            "Epoch [45/50], Loss: 0.0309\n",
            "Epoch [50/50], Loss: 0.4542\n",
            "Précision sur les données de validation: 81.11%\n",
            "Prédictions sur les données de test générées.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger les données\n",
        "train_path = '/content/train_clean.csv'\n",
        "test_path = '/content/test_clean.csv'\n",
        "\n",
        "if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_test = pd.read_csv(test_path)\n",
        "\n",
        "    # Séparation des features et labels\n",
        "    X_alltrain = df_train.drop(columns=[\"Survived\", \"PassengerId\"]).values\n",
        "    y_alltrain = df_train[\"Survived\"].values\n",
        "\n",
        "    # Séparation en train/dev\n",
        "    X_train, X_dev, y_train, y_dev = train_test_split(X_alltrain, y_alltrain, test_size=0.1, random_state=42, stratify=y_alltrain)\n",
        "\n",
        "    # Extraction des features pour le test\n",
        "    X_test = df_test.drop(columns=[\"PassengerId\"]).values\n",
        "else:\n",
        "    raise FileNotFoundError(\"Les fichiers de données ne sont pas trouvés. Assurez-vous de les télécharger dans Colab.\")\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        # Première couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Deuxième couche cachée (50 neurones) avec activation ReLU\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Couche de sortie (2 classes : Survivant ou Non)\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car CrossEntropyLoss inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "input_size = X_train.shape[1]  # Nombre de features\n",
        "learning_rate = 0.01  # Taux d'apprentissage\n",
        "num_epochs = 50  # Nombre d'époques\n",
        "batch_size = 50  # Taille du minibatch\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TitanicModel(input_size)\n",
        "print(model)\n",
        "\n",
        "# Définition de la fonction de coût (Cross Entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Définition de l'optimiseur Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Conversion des données en tenseurs PyTorch\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_dev_tensor = torch.tensor(X_dev, dtype=torch.float32)\n",
        "y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Création du DataLoader pour gérer les batchs\n",
        "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Entraînement du modèle\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Afficher la perte tous les 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Évaluation du modèle sur les données de validation\n",
        "def evaluate_model(model, X_dev, y_dev):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_dev)\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        accuracy = (predictions == y_dev).float().mean().item()\n",
        "    return accuracy\n",
        "\n",
        "dev_accuracy = evaluate_model(model, X_dev_tensor, y_dev_tensor)\n",
        "print(f\"Précision sur les données de validation: {dev_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Prédictions sur les données de test\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "\n",
        "print(\"Prédictions sur les données de test générées.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "21b294d4940218f5d2bef3782aca9a6ba6e9c054",
        "id": "PahKY7Kr3bLA"
      },
      "source": [
        "3.5 (Optionnel) Exécuter une Cross Validationd ans une boucle pour trouver les meilleures valeurs de learning rate, de keep_prob et de nombre d'epochs.\n",
        "\n",
        "Cross_Validation avec skkearn : https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyL3DrQY3bLA",
        "outputId": "e16c8e6e-0aa8-4683-c5ff-1bf047a7c2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR: 0.01, Dropout: 0.3, Accuracy: 0.8137\n",
            "LR: 0.01, Dropout: 0.5, Accuracy: 0.8171\n",
            "LR: 0.001, Dropout: 0.3, Accuracy: 0.8070\n",
            "LR: 0.001, Dropout: 0.5, Accuracy: 0.8036\n",
            "Meilleurs paramètres : {'learning_rate': 0.01, 'dropout_rate': 0.5} avec une précision de 0.8171\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "# Charger les données\n",
        "train_path = '/content/train_clean.csv'\n",
        "test_path = '/content/test_clean.csv'\n",
        "\n",
        "if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_test = pd.read_csv(test_path)\n",
        "\n",
        "    # Séparation des features et labels\n",
        "    X_alltrain = df_train.drop(columns=[\"Survived\", \"PassengerId\"]).values\n",
        "    y_alltrain = df_train[\"Survived\"].values\n",
        "\n",
        "    # Extraction des features pour le test\n",
        "    X_test = df_test.drop(columns=[\"PassengerId\"]).values\n",
        "else:\n",
        "    raise FileNotFoundError(\"Les fichiers de données ne sont pas trouvés. Assurez-vous de les télécharger dans Colab.\")\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(TitanicModel, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # Pas d'activation ici, car CrossEntropyLoss inclut softmax\n",
        "        return x\n",
        "\n",
        "# Définition des hyperparamètres\n",
        "learning_rates = [0.01, 0.001]\n",
        "num_epochs = 50\n",
        "batch_size = 50\n",
        "dropout_rates = [0.3, 0.5]\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for dropout_rate in dropout_rates:\n",
        "        fold_accuracies = []\n",
        "\n",
        "        for train_idx, val_idx in kfold.split(X_alltrain, y_alltrain):\n",
        "            X_train, X_dev = X_alltrain[train_idx], X_alltrain[val_idx]\n",
        "            y_train, y_dev = y_alltrain[train_idx], y_alltrain[val_idx]\n",
        "\n",
        "            model = TitanicModel(input_size=X_train.shape[1], dropout_rate=dropout_rate)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "            y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "            X_dev_tensor = torch.tensor(X_dev, dtype=torch.float32)\n",
        "            y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
        "\n",
        "            dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                for X_batch, y_batch in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(X_batch)\n",
        "                    loss = criterion(outputs, y_batch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = model(X_dev_tensor)\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "                accuracy = (predictions == y_dev_tensor).float().mean().item()\n",
        "                fold_accuracies.append(accuracy)\n",
        "\n",
        "        avg_accuracy = np.mean(fold_accuracies)\n",
        "        print(f\"LR: {lr}, Dropout: {dropout_rate}, Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "        if avg_accuracy > best_accuracy:\n",
        "            best_accuracy = avg_accuracy\n",
        "            best_params = {'learning_rate': lr, 'dropout_rate': dropout_rate}\n",
        "\n",
        "print(f\"Meilleurs paramètres : {best_params} avec une précision de {best_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "xpython",
      "language": "python",
      "name": "xpython"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}